{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                    (Housing Price Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sapi\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Pandas' is a software library written for the Python programming language for data manipulation and analysis. It is most widely used for data science/data analysis and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib is a python software library used for data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Sklearn'also known as 'scikit-learn' is a software machine learning library for the Python programming language which features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "#### Regression algorithm in Sklearn\n",
    "It is used to predict a continuous-valued attribute associated with an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\n",
    "\n",
    "#### statsmodels.api\n",
    "Cross-sectional models and methods. (Cross-sectional analysis looks at data collected at a single point in time, rather than over a period of time.)\n",
    "#### statsmodels.formula.api\n",
    "A convenience interface for specifying models using formula strings and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn\n",
    "Seaborn is a library in Python predominantly used for making statistical graphics. Seaborn is a data visualization library built on top of matplotlib and closely integrated with pandas data structures in Python. Visualization is the central part of Seaborn which helps in exploration and understanding of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data through 'Pandas' Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Housing Price Prediction (1).csv\")\n",
    "# Here we are getting our data of 'Housing Price Prediction' through pd.read_csv(file_name.extension) and \n",
    "#assigning to 'data' variable . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Data:\n",
    "Our data comes from dataset 'Housing Price Prediction' which is in csv(comma separated value) format. It contains 21597 rows also known as observation and 21 columns also known as features. Our aim is to predict the price of house based on the features which have major role in predicting price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)\n",
    "# Dataframe.head() tells us the topmost 5 values of the data as it's default in it we can change it by passing value in head..\n",
    "# For example \n",
    "# data.head(10)\n",
    "# It tells us topmost 10 value of the dataset..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and feature Description\n",
    "\n",
    "#### id - \n",
    "    Unique ID for each home sold\n",
    "#### date - \n",
    "    Date of the home sale\n",
    "#### price - \n",
    "    Price of each home sold\n",
    "#### bedrooms -\n",
    "    Number of bedrooms\n",
    "#### bathrooms - \n",
    "    Number of bathrooms\n",
    "#### sqft_living - \n",
    "    Square footage of the apartments interior living space\n",
    "#### sqft_lot - \n",
    "    Square footage of the land space\n",
    "#### floors -\n",
    "    Number of floors\n",
    "#### waterfront - \n",
    "    A dummy variable for whether the apartment was overlooking the waterfront or not\n",
    "#### view - \n",
    "    An index from 0 to 4 of how good the view of the property was\n",
    "#### condition - \n",
    "    An index from 1 to 5 on the condition of the apartment,\n",
    "#### grade - \n",
    "    An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n",
    "#### sqft_above - \n",
    "    The square footage of the interior housing space that is above ground level\n",
    "#### sqft_basement - \n",
    "    The square footage of the interior housing space that is below ground level\n",
    "#### yr_built - \n",
    "    The year the house was initially built\n",
    "#### yr_renovated - \n",
    "    The year of the house’s last renovation\n",
    "#### zipcode - \n",
    "    What zipcode area the house is in\n",
    "#### lat - \n",
    "    Lattitude\n",
    "#### long - \n",
    "    Longitude\n",
    "#### sqft_living15 - \n",
    "    The square footage of interior housing living space for the nearest 15 neighbors\n",
    "#### sqft_lot15 -\n",
    "    The square footage of the land lots of the nearest 15 neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Naure:\n",
    "### From the type of the features and their values count, we can determine the nature of each feature:\n",
    "\n",
    "#### Qualitative:\n",
    "\n",
    "##### Nominal: \n",
    "        id, waterfront, zipcode\n",
    "##### Ordinal: \n",
    "        date, view, condition\n",
    "\n",
    "#### Quantitative:\n",
    "\n",
    "##### Discrete: \n",
    "        bedrooms, sqft_living, sqft_lot, sqft_above, sqft_basement, yr_built, yr_renovated, sqft_living15, sqft_lot15\n",
    "        \n",
    "##### Continuous: \n",
    "        price, floors, lat, long,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "# .shape tells us the shape (number of rows, number of columns) of the dataset..\n",
    "# Our dataset (Housing Price Predicton) has 21597 rows also known as observation and 21 columns also known as features... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())\n",
    "# It tells us about our dataset if any 'Null' value is present in it columnwise.\n",
    "# In our dataset there in no 'Null' value present in it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "# .columns tells us about the all the columns aka features of the dataset.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(['yr_built'],ascending=False)\n",
    "# Here we are sorting our dataset by 'yr_built' in descending order to check our latest built houses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n",
    "# .describe() tells us the about the complete statistics about the data. In our data it includes count, mean, standard deviation,\n",
    "# and percentiles of 'Housing price prediction'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes\n",
    "# This returns a Series with the data type of each column.\n",
    "# The result’s index is the original DataFrame’s columns. Columns with mixed types are stored with the object dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast a pandas object to a specified data type\n",
    "\n",
    "data['bathrooms'] = data['bathrooms'].astype('int64')\n",
    "# it casts 'bathrooms' column to 'int64' from 'float64' \n",
    "\n",
    "data['floors'] = data['floors'].astype('int64')\n",
    "# it casts 'floors' column to 'int64' from 'float64' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()\n",
    "# corr() is used to find the pairwise correlation of all columns in the dataframe. \n",
    "# Any na values are automatically excluded. For any non-numeric data type columns in the dataframe it is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of data (Housing Price Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)\n",
    "# To plot multiple pairwise bivariate distributions in a dataset, you can use the pairplot() function. \n",
    "# This shows the relationship for combination of variable in a DataFrame as a matrix of plots and \n",
    "# the diagonal plots are the univariate plots.to plot all the values with respect to each other...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(17,11)})\n",
    "sns.heatmap(data.corr(), annot = True, cmap = 'magma')\n",
    "# Heatmap is defined as a graphical representation of data using colors to visualize the value of the matrix. \n",
    "# In this, to represent more common values or higher activities brighter colors basically reddish colors are used and \n",
    "# to represent less common or activity values, darker colors are preferred. Heatmap is also defined by the name of the shading matrix.\n",
    "# Heatmaps in Seaborn can be plotted by using the seaborn.heatmap() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.loc[:,'price'])\n",
    "# distplot() function is used to plot the distplot. The distplot represents the univariate distribution of data \n",
    "# i.e. data distribution of a variable against the density distribution. \n",
    "# The seaborn. distplot() function accepts the data variable as an argument and returns the plot with the density distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Regression \n",
    "\n",
    "Linear regression models assume that the relationship between a dependent continuous variable Y and one or more explanatory (independent) variables X is linear (that is, a straight line). It’s used to predict values within a continuous range (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). Linear regression models can be divided into two main types:\n",
    "\n",
    "#### Simple Linear Regression:\n",
    "Simple linear regression uses a traditional slope-intercept form, where a and b are the coefficients that we try to “learn” and produce the most accurate predictions. X represents our input data and Y is our prediction.\n",
    "\n",
    "##### .                                                                  Y=bX+a\n",
    "    where y: Predicted value\n",
    "    X: Predictor\n",
    "    a: Intercept (estimated by regression)\n",
    "    b: Coefficient (estimated by regression)\n",
    "\n",
    "#### Multivariable Regression\n",
    "A more complex, multi-variable linear equation might look like this, where w represents the coefficients or weights, our model will try to learn.\n",
    "#####                                        Y(x1,x2,x3.....,xn)=w0 + w1x1 + w2x2 + w3x3 +.......+ wnxn\n",
    "\n",
    "The variables x_1, x_2, x_3.......,x_n represent the attributes or distinct pieces of information, we have about each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model through Sklearn's linear_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting useful data and dividing them in dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var = data[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built']].values\n",
    "# independent variables (all columns or features except 'price','date' & 'id' )\n",
    "y_var = data['price'].values # dependent variable (it includes'price' column which we have to predict with our independent variables. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing train_test_split\n",
    "It is a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data. With this function, you don't need to divide the dataset manually. By default, Sklearn train_test_split will make random partitions for the two subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_var, y_var, test_size = 0.2, random_state = 0)\n",
    "# Here train_x,train_y are training data with 0.8(or 80%) size and test_x,test_y are included in test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model from 'sklearn's linear_model' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression()\n",
    "\n",
    "## 'LinearRegression' fits a linear model with coefficients to minimize the residual sum of squares between the\n",
    "# observed targets in the dataset, and the targets predicted by the linear approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting variables to the above built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting through the above built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "# we have assigned our all prediction of test data in y_pred variable.\n",
    "pd.Series(predictions)\n",
    "# to convert our array into pandas series.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_\n",
    "# To show coefficients of various features which helped to predict the price of house.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_\n",
    "# To show above built model's intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)\n",
    "# To show R-squared value which stands for accuracy of model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #importing numpy library..\n",
    "# NumPy is a Python library used for working with arrays. \n",
    "# It also has functions for working in domain of linear algebra, fourier transform, and matrices. \n",
    "\n",
    "plt.scatter(y_test, predictions)# to scatter 'y_test' and 'predictions'\n",
    "plt.xlabel('Actual Labels')# to x-label\n",
    "plt.ylabel('Predicted Labels')# to y-label\n",
    "plt.title('Predictions vs Actuals')# for title of graph\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "# The Numpy polyfit() method is used to fit our data inside a polynomial function, which is straight line here.\n",
    "\n",
    "p = np.poly1d(z)\n",
    "#The numpy.poly1d() function helps to define a polynomial function. \n",
    "#It makes it easy to apply “natural operations” on polynomials.\n",
    "\n",
    "plt.plot(y_test,p(y_test), color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((y_test-predictions),bins=50, color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above histogram plot, we see data is in bell shape (Normally Distributed), which means our model has done good predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2=pd.DataFrame(data=y_test,columns=['price'])\n",
    "# to create a dataframe of test_y (test price) ..\n",
    "data_2['prediction1']=predictions\n",
    "# to add predictions in above created dataframe..\n",
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing 'sklearn's mean_squared_error' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn.metrics\n",
    "The 'sklearn.metrics' module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values.\n",
    "\n",
    "#### mean_squared_error\n",
    "'mean_squared_error' function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=pd.DataFrame(data=data['price'],columns=['price'])\n",
    "data3\n",
    "# to create a dtaframe so that we can further add our predictions and compare them.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model through 'statsmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=smf.ols('price ~ id+date+bedrooms+bathrooms+sqft_living+sqft_lot+floors+waterfront+view+condition+grade+sqft_above+sqft_basement+yr_built+yr_renovated+zipcode+lat+long+sqft_living15+sqft_lot15',data=data).fit()\n",
    "# building our model thought statsmodel.formula.api's ordinary least squares(ols) on basis of feature 'id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "#'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "#'lat', 'long', 'sqft_living15', 'sqft_lot15'\n",
    "# fitting it to our data\n",
    "\n",
    "data3['ypred_f'] = model1.predict(data[['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "       'lat', 'long', 'sqft_living15', 'sqft_lot15']])\n",
    "\n",
    "# predicting value with the above built model and features used in building that model\n",
    "\n",
    "model1.summary()\n",
    "# to show all the details about above built model..\n",
    "MSE7 = np.square(np.subtract(data['price'],data3['ypred_f'])).mean()\n",
    "MSE7\n",
    "# To show mean squared error using numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=smf.ols('price ~ sqft_living+grade+sqft_above+sqft_living15+bathrooms+view',data=data).fit()\n",
    "# building our model thought statsmodel.formula.api's ordinary least squares(ols) on basis of feature 'sqft_living','grade',\n",
    "# 'sqft_above','sqft_living15','bathrooms'and 'view' and fitting it to our data\n",
    "data3['ypred_4'] = model2.predict(data[['sqft_living','grade','sqft_above','sqft_living15','bathrooms','view']])\n",
    "# predicting value with the above built model and features used in building that model\n",
    "model2.summary()\n",
    "# to show all the details about above built model..\n",
    "MSE2 = np.square(np.subtract(data['price'],data3['ypred_4'])).mean()\n",
    "MSE2\n",
    "# To show mean squared error using numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=smf.ols('price ~ sqft_living+grade+sqft_above+sqft_living15+bathrooms+view+sqft_basement+bedrooms',data=data).fit()\n",
    "# building our model thought statsmodel.formula.api's ordinary least squares(ols) on basis of feature  'sqft_living','grade',\n",
    "# 'sqft_above','sqft_living15','bathrooms','view', 'sqft_basement' and 'bedrooms' and fitting it to our data\n",
    "data3['ypred_3'] = model3.predict(data[['sqft_living','grade','sqft_above','sqft_living15','bathrooms','view','sqft_basement','bedrooms']])\n",
    "# predicting value with the above built model and features used in building that model\n",
    "model3.summary()\n",
    "# to show all the details about above built model..\n",
    "MSE3 = np.square(np.subtract(data['price'],data3['ypred_3'])).mean()\n",
    "MSE3\n",
    "# To show mean squared error using numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4=smf.ols('price ~ sqft_living+grade+sqft_above+sqft_living15+bathrooms+view+sqft_basement+bedrooms+lat',data=data).fit()\n",
    "# building our model thought statsmodel.formula.api's ordinary least squares(ols) on basis of feature sqft_living,'grade','sqft_above','sqft_living15',\n",
    "# 'bathrooms','view','sqft_basement','bedrooms','lat' and fitting it to our data\n",
    "data3['ypred_2'] = model4.predict(data[['sqft_living','grade','sqft_above','sqft_living15','bathrooms','view','sqft_basement','bedrooms','lat']])\n",
    "\n",
    "# predicting value with the above built model and features used in building that model\n",
    "model4.summary()\n",
    "# to show all the details about above built model..\n",
    "MSE4 = np.square(np.subtract(data['price'],data3['ypred_2'])).mean()\n",
    "MSE4\n",
    "# To show mean squared error using numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5=smf.ols('price ~ sqft_living+grade+sqft_above+sqft_living15+bathrooms+view+sqft_basement+bedrooms+lat+yr_renovated+id+floors+waterfront',data=data).fit()\n",
    "# building our model thought statsmodel.formula.api's ordinary least squares(ols) on basis of feature 'sqft_living','grade','sqft_above',\n",
    "#'sqft_living15','bathrooms','view','sqft_basement','bedrooms','lat','yr_renovated','id','floors','waterfront' and \n",
    "# fitting it to our data\n",
    "data3['ypred_1'] = model5.predict(data[['sqft_living','grade','sqft_above','sqft_living15','bathrooms','view','sqft_basement','bedrooms','lat','yr_renovated','id','floors','waterfront']])\n",
    "# predicting value with the above built model and features used in building that model\n",
    "model5.summary()\n",
    "# to show all the details about above built model..\n",
    "MSE5 = np.square(np.subtract(data['price'],data3['ypred_1'])).mean()\n",
    "MSE5\n",
    "# To show mean squared error using numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6=smf.ols('price ~ sqft_living+lat+bathrooms+yr_built+bedrooms+grade+floors+view+condition+waterfront+long',data=data).fit()\n",
    "# building our model thought statsmodel.formula.api's ordinary least squares(ols) on basis of feature 'sqft_living','lat',\\\n",
    "# 'bathrooms','yr_built','bedrooms','grade','floors','view','condition','waterfront','long' and\n",
    "# fitting it to our data\n",
    "\n",
    "data3['ypred'] = model6.predict(data[['sqft_living','lat','bathrooms','yr_built','bedrooms','grade','floors','view','condition','waterfront','long']])\n",
    "\n",
    "# predicting value with the above built model and features used in building that model\n",
    "\n",
    "model6.summary()\n",
    "# to show all the details about above built model..\n",
    "MSE6 = np.square(np.subtract(data['price'],data3['ypred'])).mean()\n",
    "MSE6\n",
    "# To show mean squared error using numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7=smf.ols('price ~ id+bedrooms+bathrooms+sqft_living+sqft_lot+waterfront+view+condition+grade+sqft_above+sqft_basement+yr_built+yr_renovated+zipcode+lat+long+sqft_living15+sqft_lot15',data=data).fit()\n",
    "# building our model thought statsmodel.formula.api's ordinary least squares(ols) on basis of feature 'id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "#'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "#'lat', 'long', 'sqft_living15', 'sqft_lot15'\n",
    "# fitting it to our data\n",
    "\n",
    "data3['ypred_f'] = model7.predict(data[['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "       'lat', 'long', 'sqft_living15', 'sqft_lot15']])\n",
    "\n",
    "# predicting value with the above built model and features used in building that model\n",
    "\n",
    "model7.summary()\n",
    "# to show all the details about above built model..\n",
    "MSE7 = np.square(np.subtract(data['price'],data3['ypred_f'])).mean()\n",
    "MSE7\n",
    "# To show mean squared error using numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['price'], data3['ypred_f'])# to scatter 'y_test' and 'predictions'\n",
    "plt.xlabel('Actual Labels')# to x-label\n",
    "plt.ylabel('Predicted Labels')# to y-label\n",
    "plt.title('Predictions vs Actuals')# for title of graph\n",
    "z1 = np.polyfit(data['price'], data3['ypred_f'], 1)\n",
    "# The Numpy polyfit() method is used to fit our data inside a polynomial function.\n",
    "\n",
    "p1 = np.poly1d(z1)\n",
    "\"\"\"\n",
    "The numpy.poly1d() function helps to define a polynomial function. \n",
    "It makes it easy to apply “natural operations” on polynomials.\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(data['price'],p(data['price']), color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, on the basis of various features' p-values our 'model6' has maximum accuracy of about 69.5% so our further visualisations would be on the basis of it..... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing 'plotly' library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotly\n",
    "Plotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures.Plotly Express provides functions to visualize a variety of types of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of individual feature's regression model with price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variable=['sqft_living','lat','bathrooms','sqft_living15','bedrooms','grade','floors','view','sqft_basement','waterfront','sqft_above']\n",
    "# making a list of features which helped in getting maximum accuracy and assigning it to variable. \n",
    "for i in variable:\n",
    "    fig2 = px.scatter(\n",
    "    data, x=data.loc[:,i], y='price', opacity=0.65,\n",
    "    trendline='ols', trendline_color_override='darkblue',\n",
    "    title=\"Linear Regression of feature with price\"\n",
    "    )\n",
    "    fig2.show()\n",
    "# running a for loop for item in list 'variable' and visualise scattering of these features and their regression line using \n",
    "# plotly.express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have seen a lot of cases and models for our 'Housing Price Prediction' dataset and from these cases we have three best cases. These are :\n",
    "\n",
    "#### Case 1:\n",
    "\n",
    "##### Our first model built through 'statmodel' library's 'OLS(Ordinary least square )' method which is about 71% accurate and having mean-squared error around 39193614215.43002 which is on the basis of independent variables or features on basis of  all of the features except price of house ( 'id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15','floors' and 'sqft_lot15' ).\n",
    "\n",
    "#### Case 2:\n",
    "\n",
    "##### Our last model built through 'statmodel' library's 'OLS(Ordinary least square )' method which is about 70% accurate and having mean-squared error around 39193614215.43002 which is on the basis of independent variables or features on basis of feature 'id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', and 'sqft_lot15' (these variable are selected by their correlation with price of house and p-values of their model's summary).\n",
    "\n",
    "#### Case 3:\n",
    "\n",
    "##### Our first model built through sklearn library which is about  66% accurate and having mean-squarerd error around 40212974895.048164.\n",
    "\n",
    "#### Hence, our three of the models are perfect but our first preference will be our models in case:1 and case:2 becuase they are better than the model in case:3 and having more accuracy and less error than the model in case:3....  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
